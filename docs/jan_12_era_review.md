1) Разбор дочернего воркфлоу DownloadWorkflow
Расположение: test_ideas/download_workflow.py

Назначение: найти «сырые» артефакты по scan_id, при необходимости отфильтровать по kinds, скачать их из S3 и вернуть локальные пути по типам (kind).

Шаги выполнения
Инициализация и прогресс

В __init__ задаются _stage = 'Initialize' и _scan_id = None.

progress() возвращает текущую стадию и scan_id.

Поиск артефактов

Устанавливает _scan_id и _stage = 'Looking for raw artifacts for scan_id in DB'.

Activity list_raw_artifacts получает список артефактов по scan_id (таймаут 30 сек, до 3 попыток).

Фильтрация по kinds (опционально)

Если params.kinds задан, фильтрует список raw_arts по указанным типам.

Ошибки при пустом наборе

Если список пуст — выбрасывает ApplicationError с non_retryable=True.

Скачивание артефактов

_stage = 'Download artifacts'.

Для каждого артефакта вызывает download_from_s3 (таймаут 1 час, максимум 2 попытки).

Проверяет дубликаты kind и падает с ApplicationError, если встречается повтор.

Складывает локальные пути в results по ключу kind.

Завершение

_stage = 'Workflow is done'.

Возвращает словарь {kind: local_path}.

2) Оценка и точки роста
A) DownloadWorkflow — сильные стороны
Простая и читаемая логика: всё в одном месте, понятно, какие стадии есть и какие активности вызываются.

Базовая наблюдаемость: стадия и scan_id доступны через query progress.

Контроль ошибок: явные non-retryable ошибки для пустого списка и дубликатов kind.

A) DownloadWorkflow — куда расти
Конфигурация версии

Версия workflow берётся из жёстко заданного пути D:\1_prod\point_cloud\config.yaml — это Windows-специфично и ломает переносимость. Лучше получать через env/config в стиле остальных модулей или через параметры запуска воркфлоу.

Неиспользуемые импорты

settings, Repo, S3Store импортированы, но не используются — это признак устаревшего/чернового состояния и потенциальной путаницы для поддержки.

Параллельная загрузка

Сейчас скачивание идет последовательно в цикле. Если raw_arts может быть больше одного, можно параллелить через workflow.execute_activity + asyncio.gather и ограничитель конкурентности — это сократит общее время.

Более детализированный прогресс

Сейчас _stage обновляется только крупными этапами. Можно добавить прогресс “X из N скачано” или текущий kind, чтобы улучшить UX/наблюдаемость.

Гибкая политка retry

maximum_attempts=2 для S3 может быть маловато для нестабильных сетей; стоит вынести это в настройки.

B) ProfilingWorkflow — сильные стороны
Четкий пайплайн этапов: скачивание → профилирование → чтение hexbin → извлечение полей → загрузка → манифест.

Использование child workflow для скачивания данных — это повышает переиспользуемость и изоляцию ответственности (Download отдельно, profiling отдельно).

Стадии прогресса помогают отслеживать выполнение на уровне workflow.

B) ProfilingWorkflow — куда расти
Возврат результата

Workflow заканчивается установкой _stage = 'Aggregating results', но не возвращает итог (manifest_info вообще не используется). Можно вернуть метаданные/manifest info как результат, чтобы downstream сервисы могли его напрямую использовать.

Конфигурация версии

Аналогично DownloadWorkflow, версия читается из жёсткого пути D:\1_prod\point_cloud\config.yaml, что не переносимо в Linux/CI/контейнере.

Retry policy

Все активности в profiling ограничены maximum_attempts=1. Это безопасно, но снижает устойчивость к временным ошибкам чтения/сети. Обычно полезно дать хотя бы 2–3 попытки и backoff для чтения/загрузки.

Проверки валидности входов

Нет проверок на существование raw.point_cloud в files_by_kind до обращения files_by_kind["raw.point_cloud"]. При ошибках на шаге скачивания будет исключение KeyError, которое не даёт понятного сообщения — лучше явно проверить и выдать ApplicationError/ValueError с контекстом.

Типы и структура результата

meta, hexbin_fields, upload_info, manifest_info имеют неявные типы. Можно описать dataclass/TypedDict, чтобы закрепить контракт между activities и workflow.

3) Связь двух воркфлоу
ProfilingWorkflow вызывает DownloadWorkflow как child workflow и ожидает ровно один kind — "raw.point_cloud". Это жесткая связка и подразумевает, что в хранилище всегда есть этот тип, иначе будет KeyError после получения результата. При расширении типов стоит синхронизировать список kinds и обработку ошибок.